{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c7e40f-dad0-4a24-9aa2-ce856d5e7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "# OS interaction and time\n",
    "import os\n",
    "import sys\n",
    "import cftime\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import calendar\n",
    "\n",
    "# math and data\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "from scipy.signal import detrend\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "from sklearn import linear_model\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "# random\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362c4b83-84bc-4cbf-9f06-b2fecffd4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_era5_path = '/glade/u/home/zcleveland/scratch/ERA5/'  # path to subset data\n",
    "misc_data_path = '/glade/u/home/zcleveland/scratch/misc_data/'  # path to misc data\n",
    "sub_script_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/scripts/subsetting/'  # path to subsetting scripts\n",
    "plot_script_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/scripts/plotting/'  # path to plotting scripts\n",
    "plot_out_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/plots/'  # path to generated plots\n",
    "temp_scratch_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/temp/'  # path to temp directory in scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c5ff3a-e79d-4001-880c-6dd47012d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable lists\n",
    "# surface instantaneous variables\n",
    "sfc_instan_list = [\n",
    "    'sd',  # snow depth  (m of water equivalent)\n",
    "    'msl',  # mean sea level pressure (Pa)\n",
    "    'tcc',  # total cloud cover (0-1)\n",
    "    'stl1',  # soil temp layer 1 (K)\n",
    "    'stl2',  # soil temp layer 2 (K)\n",
    "    'stl3',  # soil temp layer 3 (K)\n",
    "    'stl4',  # soil temp layer 4 (K)\n",
    "    'swvl1',  # soil volume water content layer 1 (m^3 m^-3)\n",
    "    'swvl2',  # soil volume water content layer 2 (m^3 m^-3)\n",
    "    'swvl3',  # soil volume water content layer 3 (m^3 m^-3)\n",
    "    'swvl4',  # soil volume water content layer 4 (m^3 m^-3)\n",
    "    '2t',  # 2 meter temp (K)\n",
    "    '2d',  # 2 meter dew point (K)\n",
    "    'ishf',  # instant surface heat flux (W m^-2)\n",
    "    'ie',  # instant moisture flux (kg m^-2 s^-1)\n",
    "    'cape',  # convective available potential energy (J kg^-1)\n",
    "    'tcw',  # total column water (kg m^-2) -- sum total of solid, liquid, and vapor in a column\n",
    "    'sstk',  # sea surface temperature (K)\n",
    "    'vipile',  # vertical integral of potential, internal, and latent energy (J m^-2)\n",
    "    'viwve',  # vertical integral of eastward water vapour flux (kg m^-1 s^-1) - positive south -> north\n",
    "    'viwvn',  # vertical integral of northward water vapour flux (kg m^-1 s^-1) - positive west -> east\n",
    "    'viwvd',  # vertical integral of divergence of moisture flux (kg m^-2 s^-1) - positive divergencve\n",
    "    'z_thick_1000-500',  # geopotential height thickness (m) - difference between two height levels\n",
    "]\n",
    "\n",
    "# surface accumulation variables\n",
    "sfc_accumu_list = [\n",
    "    'lsp',  # large scale precipitation (m of water)\n",
    "    'cp',  # convective precipitation (m of water)\n",
    "    'tp',  # total precipitation (m of water) -- DERIVED\n",
    "    'sshf',  # surface sensible heat flux (J m^-2)\n",
    "    'slhf',  # surface latent heat flux (J m^-2)\n",
    "    'ssr',  # surface net solar radiation (J m^-2)\n",
    "    'str',  # surface net thermal radiation (J m^-2)\n",
    "    'sro',  # surface runoff (m)\n",
    "    'sf',  # total snowfall (m of water equivalent)\n",
    "    'ssrd',  # surface solar radiation downwards (J m^-2)\n",
    "    'strd',  # surface thermal radiation downwards (J m^-2)\n",
    "    'ttr',  # top net thermal radiation (OLR, J m^-2) -- divide by time (s) for W m^-2\n",
    "]\n",
    "\n",
    "# pressure level variables\n",
    "pl_var_list = [\n",
    "    # 'pv',  # potential vorticity (K m^2 kg^-1 s^-1)\n",
    "    # 'crwc',  # specific rain water content (kg kg^-1)\n",
    "    # 'cswc',  # specific snow water content (kg kg^-1)\n",
    "    'z',  # geopotential (m^2 s^2)\n",
    "    'z_height',  # geopotential height (m)\n",
    "    't',  # temperature (K)\n",
    "    'u',  # u component of wind(m s^-1)\n",
    "    'v',  # v component of wind (m s^-1)\n",
    "    'q',  # specific humidity (kg kg^-1)\n",
    "    'w',  # vertical velo|city (Pa s^-1)\n",
    "    # 'vo',  # vorticity - relative (s^-1)\n",
    "    # 'd',  # divergence (s^-1)\n",
    "    'r',  # relative humidity (%)\n",
    "    # 'clwc',  # specific cloud liquid water content\n",
    "    # 'ciwc',  # specific cloud ice water content\n",
    "    # 'cc',  # fraction of cloud cover (0-1)\n",
    "]\n",
    "\n",
    "# invariant data\n",
    "invar_var_list = [\n",
    "    'cl',  # lake cover (0-1)\n",
    "    'dl',  # lake depth (m)\n",
    "    'cvl',  # low vegetation cover (0-1)\n",
    "    'cvh',  # high vegetation cover (0-1)\n",
    "    'tvl',  # type of low vegetation ~\n",
    "    'tvh',  # type of high begetation ~\n",
    "    'slt',  # soil type ~\n",
    "    'sdfor',  # standard deviation of filtered subgrid orography (m)\n",
    "    'z_sfc',  # geopotential of surface (m^2 s^-2)\n",
    "    'sdor',  # standard deviation of orography ~\n",
    "    'isor',  # anisotropy of subgridscale orography ~\n",
    "    'anor',  # angle of subgridscale orography (radians)\n",
    "    'slor',  # slope of subgridscale orography ~\n",
    "    'lsm',  # land-sea mask (0-1)\n",
    "    'elevation',  # elevation of terrain (m)\n",
    "]\n",
    "\n",
    "# NAM variables\n",
    "NAM_var_list = [\n",
    "    'onset',\n",
    "    'retreat',\n",
    "    'length',\n",
    "    'precipitation',\n",
    "    'precipitation-rate'\n",
    "]\n",
    "\n",
    "# all var in one list\n",
    "var_list = sfc_instan_list + sfc_accumu_list + pl_var_list\n",
    "\n",
    "# region average list\n",
    "region_avg_list = [\n",
    "    'cp',\n",
    "    'mr',\n",
    "    'son',\n",
    "    'chi',\n",
    "    'moj',\n",
    "    'MeNmAz',\n",
    "]\n",
    "\n",
    "# variables that are fluxes and need to be multiplied by -1 for easier understanding\n",
    "flux_var_list = [\n",
    "    'sshf',  # surface sensible heat flux (J m^-2)\n",
    "    'slhf',  # surface latent heat flux (J m^-2)\n",
    "    'ttr',  # top net thermal radiation (OLR, J m^-2) -- divide by time (s) for W m^-2\n",
    "    'ishf',  # instant surface heat flux (W m^-2)\n",
    "    'ie',  # instant moisture flux (kg m^-2 s^-1)\n",
    "    'str',  # surface thermal radiation (J m^-2)\n",
    "]\n",
    "\n",
    "# misc variables\n",
    "misc_var_list = [\n",
    "    'nino-3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01911523-4fd7-4780-a9f9-8b7b63cfb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable dictionaries\n",
    "\n",
    "# dictionary of variables and their names\n",
    "var_dict = {\n",
    "    'sd': 'Snow Depth',\n",
    "    'msl': 'Mean Sea Level Pressure',\n",
    "    'tcc': 'Total Cloud Cover',\n",
    "    'stl1': 'Soil Temp Layer 1',\n",
    "    'stl2': 'Soil Temp Layer 2',\n",
    "    'stl3': 'Soil Temp Layer 3',\n",
    "    'stl4': 'Soil Temp Layer 4',\n",
    "    'swvl1': 'Soil Volume Water Content Layer 1',\n",
    "    'swvl2': 'Soil Volume Water Content Layer 2',\n",
    "    'swvl3': 'Soil Volume Water Content Layer 3',\n",
    "    'swvl4': 'Soil Volume Water Content Layer 4',\n",
    "    '2t': '2 Meter Temp',\n",
    "    '2d': '2 Meter Dew Point',\n",
    "    'ishf': 'Instant Surface Heat Flux',\n",
    "    'ie': 'Instant Moisture Flux',\n",
    "    'cape': 'Convective Available Potential Energy',\n",
    "    'tcw': 'Total Column Water',\n",
    "    'sstk': 'Sea Surface Temperature',\n",
    "    'vipile': 'vertical integral of potential, internal, and latent energy',\n",
    "    'viwve': 'vertical integral of eastward water vapour flux',\n",
    "    'viwvn': 'vertical integral of northward water vapour flux',\n",
    "    'viwvd': 'vertical integral of divergence of moisture flux',\n",
    "    'lsp': 'Large Scale Precipitation',\n",
    "    'cp': 'Convective Precipitation',\n",
    "    'tp': 'Total Precipitation',\n",
    "    'sshf': 'Surface Sensible Heat Flux',\n",
    "    'slhf': 'Surface Latent Heat Flux',\n",
    "    'ssr': 'Surface Net Solar Radiation',\n",
    "    'str': 'Surface Net Thermal Radiation',\n",
    "    'sro': 'Surface Runoff',\n",
    "    'sf': 'Total Snowfall',\n",
    "    'ssrd': 'Surface Solar Radiation Downwards',\n",
    "    'strd': 'Surface Thermal Radiation Downwards',\n",
    "    'ttr': 'Top Net Thermal Radiation (OLR)',\n",
    "    'z': 'Geopotential',\n",
    "    'z_height': 'Geopotential Height',\n",
    "    'z_thick': 'Geopotential Height Thickness',\n",
    "    't': 'Temperature',\n",
    "    'u': 'U Component of Wind',\n",
    "    'v': 'V Component of Wind',\n",
    "    'q': 'Specific Humidity',\n",
    "    'w': 'Vertical Velocity',\n",
    "    'r': 'Relative Humidity',\n",
    "    'onset': 'NAM Onset',\n",
    "    'retreat': 'NAM Retreat',\n",
    "    'length': 'NAM Length',\n",
    "    'precipitation': 'Yearly NAM Season Precipitation',\n",
    "    'precipitation-rate': 'NAM Precipitation Rate',\n",
    "    'nino-3': r'Ni$\\tilda{n}$o-3 Index',\n",
    "}\n",
    "\n",
    "# variable units in latex format for plotting\n",
    "var_units = {\n",
    "    'sd': r'(m)',\n",
    "    'msl': r'(Pa)',\n",
    "    'tcc': r'(0-1)',\n",
    "    'stl1': r'(K)',\n",
    "    'stl2': r'(K)',\n",
    "    'stl3': r'(K)',\n",
    "    'stl4': r'(K)',\n",
    "    'swvl1': r'$(m^3 m^{-3})$',\n",
    "    'swvl2': r'$(m^3 m^{-3})$',\n",
    "    'swvl3': r'$(m^3 m^{-3})$',\n",
    "    'swvl4': r'$(m^3 m^{-3})$',\n",
    "    '2t': r'(K)',\n",
    "    '2d': r'(K)',\n",
    "    'ishf': r'$(W m^{-2})$',\n",
    "    'ie': r'$(kg m^{-2} s^{-1})$',\n",
    "    'cape': r'$(J kg^{-1})$',\n",
    "    'tcw': r'$(kg m^{-2})$',\n",
    "    'sstk': r'(K)',\n",
    "    'vipile': r'$(J m^{-2})$',\n",
    "    'viwve': r'$(kg m^{-1} s^{-1})$',\n",
    "    'viwvn': r'$(kg m^{-1} s^{-1})$',\n",
    "    'viwvd': r'$(kg m^{-2} s^{-1})$',\n",
    "    'lsp': r'(m)',\n",
    "    'cp': r'(m)',\n",
    "    'tp': r'(m)',\n",
    "    'sshf': r'$(J m^{-2})$',\n",
    "    'slhf': r'$(J m^{-2})$',\n",
    "    'ssr': r'$(J m^{-2})$',\n",
    "    'str': r'$(J m^{-2})$',\n",
    "    'sro': r'(m)',\n",
    "    'sf': r'(m)',\n",
    "    'ssrd': r'$(J m^{-2})$',\n",
    "    'strd': r'$(J m^{-2})$',\n",
    "    'ttr': r'$(J m^{-2})$',\n",
    "    'z': r'$(m^2 s^{-2})$',\n",
    "    'z_height': '$(m)$',\n",
    "    'z_thick': '$(m)$',\n",
    "    't': r'(K)',\n",
    "    'u': r'$(m s^{-1})$',\n",
    "    'v': r'$(m s^{-1})$',\n",
    "    'q': r'$(kg kg^{-1})$',\n",
    "    'w': r'$(Pa s^{-1})$',\n",
    "    'r': r'(%)',\n",
    "    'onset': '',\n",
    "    'retreat': '',\n",
    "    'length': r'# of days',\n",
    "    'precipitation': r'(m)',\n",
    "    'precipitation-rate': r'(m day^{-1}, NAM Season Precip / NAM Length)',\n",
    "    'nino-3': r'(Ni$\\tilda{n}$o-3 Index Anomaly)',\n",
    "}\n",
    "\n",
    "# dictionary of regions and their names\n",
    "region_avg_dict = {\n",
    "    'cp': 'Colorado Plateau',\n",
    "    'mr': 'Mogollon Rim',\n",
    "    'son': 'Sonoran Desert',\n",
    "    'chi': 'Chihuahuan Desert',\n",
    "    'moj': 'Mojave Desert',\n",
    "    'MeNmAz': 'MEX, NM, AZ Border',\n",
    "    'baja': r'Coast of Baja, CA (5$\\degree$ x 5$\\degree$)',\n",
    "}\n",
    "\n",
    "# dictionary of regions and their coordinate boundaries\n",
    "# [WEST, EAST, NORTH, SOUTH] -- WEST and EAST are on 0-360 latitude grid system\n",
    "region_avg_coords = {\n",
    "    'cp': [249, 253, 39, 35],\n",
    "    'mr': [249, 251, 33, 34],\n",
    "    'son': [246, 250, 28, 32],\n",
    "    'chi': [252, 256, 29, 33],\n",
    "    'moj': [243, 247, 33, 37],\n",
    "    'MeNmAz': [246, 256, 38, 28],\n",
    "    'baja': [242, 247, 27, 22],\n",
    "}\n",
    "\n",
    "# dictionary of colors for the plot of each region\n",
    "region_colors_dict = {\n",
    "    'cp': 'blue',\n",
    "    'mr': 'darkorange',\n",
    "    'son': 'green',\n",
    "    'chi': 'red',\n",
    "    'moj': 'purple',\n",
    "    'MeNmAz': 'brown',\n",
    "    'baja': 'yellow',\n",
    "    'dsw': 'black'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87def62f-a6fc-4d98-a91c-19cdfcc07642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get var files, open dataset, and subset if needed\n",
    "def get_var_data(var, region='dsw', months=[i for i in range(1,13)], **kwargs):\n",
    "    r\"\"\"\n",
    "    Retrieves the data for a given variable from my subet ERA5 dataset.  User can choose to return a dataset or data array\n",
    "    and whether to subset that data based on a region or time.  Any subset data is returned as a data array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var : str\n",
    "            The variable desired\n",
    "    region : str\n",
    "            The region desired\n",
    "    months : list, int\n",
    "            A list of months desired [1, 2, ..., 12]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    var_data : xarray Data Array\n",
    "            A data array containing the desired data, either in full or subset based on user input\n",
    "\n",
    "    Kwargs\n",
    "    ------\n",
    "    subset_flag : bool\n",
    "            True or False.  Whether to subset the data or not\n",
    "    level : int\n",
    "            The pressure level desired.  Only applied for pressure level data\n",
    "    type : str\n",
    "            Specify whether to return a dataset or data array\n",
    "    mean_flag : bool\n",
    "            True or False.  Whether to compute the mean (or sum) over the specified months\n",
    "    group_type : str\n",
    "            How to group data prior to computing mean or sum across time.\n",
    "            Options include 'year', 'month', 'dayofyear', etc.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    get_var_files : returns all files for specified variable\n",
    "    open_var_data : opens the variable dataset or data array\n",
    "    subset_var_data : subsets data array based on user input\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files = get_var_files(var, region, **kwargs)\n",
    "    var_data = open_var_data(files, var, **kwargs)\n",
    "    if kwargs.get('subset_flag', False):\n",
    "        return subset_var_data(var_data, var, months, region, **kwargs)\n",
    "    return var_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b59a3e-b868-4650-9a7d-8ddf2ff96f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the files for a given variable/region\n",
    "def get_var_files(var, region, **kwargs):\n",
    "\n",
    "    # grab files for sfc var\n",
    "    if ((var in sfc_instan_list) or (var in sfc_accumu_list)):\n",
    "        # dsw\n",
    "        if region != 'global':\n",
    "            files = glob.glob(f'{my_era5_path}dsw/*/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "        elif region == 'global':\n",
    "            files = glob.glob(f'{my_era5_path}global/*/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "    # grab files for pl var\n",
    "    elif var in pl_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}dsw/*/pl/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "    # grab files for NAM var\n",
    "    elif var in NAM_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}dsw/NAM_{var}.nc')\n",
    "\n",
    "    elif var in misc_var_list:\n",
    "        files = glob.glob(f'{misc_data_path}{var}/{var}*.nc')\n",
    "\n",
    "    elif var in invar_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}invariants/{var}_invariant.nc')\n",
    "\n",
    "    # if something went wrong\n",
    "    else:\n",
    "        print('something went wrong finding files')\n",
    "        files = []\n",
    "\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bbdca9-3ff8-4aaa-9240-44d9f99bf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to open variable datasets\n",
    "def open_var_data(files, var, **kwargs):\n",
    "    # get kwargs\n",
    "    type = kwargs.get('type', 'da')  # default to returning a data array\n",
    "\n",
    "    # open dataset\n",
    "    ds = xr.open_mfdataset(files)\n",
    "    if type == 'ds':\n",
    "        return ds\n",
    "\n",
    "    # pull out actual variable name in the dataset since they can be different names/capitalized\n",
    "    var_name = [v for v in ds.data_vars.keys() if f'{var.upper()}' in v.upper()][0]\n",
    "    return ds[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae9994f-6b8f-4836-8cbf-364298b03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to open subset an input data set (or array) by:\n",
    "# latitude/longitude\n",
    "# time\n",
    "# averages\n",
    "def subset_var_data(var_data, var, months, region, **kwargs):\n",
    "\n",
    "    if isinstance(var_data, xr.Dataset):\n",
    "        # pull out actual variable name in the dataset since they can be different names/capitalized\n",
    "        var_name = [v for v in var_data.data_vars.keys() if f'{var.upper()}' in v.upper()][0]\n",
    "        da = var_data[var_name]\n",
    "    elif isinstance(var_data, xr.DataArray):\n",
    "        da = var_data\n",
    "    else:\n",
    "        print('something wrong with var_data in subset_var_data')\n",
    "        return None\n",
    "\n",
    "    # subset to regional average if region is specified\n",
    "    if region in region_avg_list:\n",
    "        lats = slice(region_avg_coords[region][2], region_avg_coords[region][3])\n",
    "        lons = slice(region_avg_coords[region][0], region_avg_coords[region][1])\n",
    "        da = da.sel(latitude=lats, longitude=lons).mean(dim=['latitude', 'longitude'], skipna=True)\n",
    "\n",
    "    # subset to level if var is a pl var\n",
    "    if var.lower() in pl_var_list:\n",
    "        level = kwargs.get('level', None)\n",
    "        if level is not None:\n",
    "            da = da.sel(level=level)\n",
    "\n",
    "    # just return da if var is NAM var, convert to dayofyear for onset and retreat dates\n",
    "    if var.lower() in NAM_var_list:\n",
    "        if ((var.lower() == 'onset') or (var.lower() == 'retreat')):\n",
    "            return da.dt.dayofyear\n",
    "        else:\n",
    "            return da\n",
    "\n",
    "    # subset the data specified by months\n",
    "    da_sub = da.sel(time=da['time.month'].isin(months))\n",
    "\n",
    "    # subset further and compute mean/sum if specified by mean_flag\n",
    "    mean_flag = kwargs.get('mean_flag', False)\n",
    "    if mean_flag:\n",
    "        groupby_type = f\"time.{kwargs.get('group_type', 'year')}\"\n",
    "\n",
    "        if var.lower() in sfc_accumu_list:\n",
    "            return da_sub.groupby(groupby_type).sum(dim='time')\n",
    "        else:\n",
    "            return da_sub.groupby(groupby_type).mean(dim='time')\n",
    "\n",
    "    # if mean_flag is False, jsut return whole data array\n",
    "    else:\n",
    "        return da_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c018f335-0dfe-4c75-8365-c02f1fe9f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if inputs are list or not\n",
    "def ensure_var_list(x):\n",
    "\n",
    "    if not isinstance(x, list):\n",
    "        return [x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117b7822-643e-4eb7-8ca5-8cb0c812866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to turn a list of integers into months\n",
    "def month_num_to_name(var, months, **kwargs):\n",
    "\n",
    "    # make string for month letters from var_range (e.g. [6,7,8] -> 'JJA')\n",
    "    if var in NAM_var_list:\n",
    "        var_months = ''  # don't use months for onset, retreat, length\n",
    "    elif len(months) == 1:\n",
    "        var_months = calendar.month_name[months[0]]  # use full month name if only 1 month\n",
    "    elif len(months) == 12 and kwargs.get('mean_flag', True):\n",
    "        var_months = 'YEAR'\n",
    "    elif ((len(months) > 1) & (len(months) <= 12)):\n",
    "        var_months = ''.join([calendar.month_name[m][0] for m in months])  # make string of months, i.e. 3, 4, 5 is MAM\n",
    "    return var_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba2642c-cec4-462d-bc07-416e2cca18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to detrend the data\n",
    "\n",
    "# MANUALLY DETREND WITH LINEAR REGRESSION\n",
    "def detrend_data(arr):\n",
    "\n",
    "    # set up x array for the years\n",
    "    arr_time = np.arange(0,len(arr))\n",
    "\n",
    "    # mask out nan values\n",
    "    mask = np.isfinite(arr)\n",
    "    arr_time_mask = arr_time[mask]\n",
    "    arr_mask = arr[mask]\n",
    "\n",
    "    # make sure the array is not full of non-finite values\n",
    "    if len(arr_mask) == 0:\n",
    "        arr_detrend = np.empty(len(arr))\n",
    "        arr_detrend[:] = np.nan\n",
    "\n",
    "    else:\n",
    "        # compute linear regression\n",
    "        result = sp.stats.linregress(arr_time_mask, arr_mask)\n",
    "        m, b = result.slope, result.intercept\n",
    "\n",
    "        # detrend the data\n",
    "        arr_detrend = arr - (m*arr_time + b)\n",
    "\n",
    "    return arr_detrend\n",
    "\n",
    "\n",
    "# define a function to mask data for detrending or correlating\n",
    "def apply_detrend(da, **kwargs):\n",
    "\n",
    "    input_dims = kwargs.get('input_dims', 'time')\n",
    "    # load data\n",
    "    da.load()\n",
    "\n",
    "    da_detrend = xr.apply_ufunc(\n",
    "        detrend_data, da,\n",
    "        input_core_dims=[[input_dims]],\n",
    "        output_core_dims=[[input_dims]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[da.dtype]\n",
    "    )\n",
    "\n",
    "    return da_detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a613208-7a01-44b4-8fbb-4901294445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to regress data\n",
    "def regress_data(arr1, arr2):\n",
    "\n",
    "    # mask out nan values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    arr1_mask = arr1[mask]\n",
    "    arr2_mask = arr2[mask]\n",
    "\n",
    "    if len(arr1_mask) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    else:\n",
    "        # compute linear regression\n",
    "        res = sp.stats.linregress(arr1_mask, arr2_mask)\n",
    "        return res.slope, res.intercept, res.rvalue, res.pvalue, res.stderr, res.intercept_stderr\n",
    "\n",
    "\n",
    "# define a function to mask data for detrending or correlating\n",
    "def apply_regression(da1, da2, **kwargs):\n",
    "\n",
    "    input_dims = kwargs.get('input_dims', 'time')\n",
    "    # load data\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "\n",
    "    result = xr.apply_ufunc(\n",
    "        regress_data, da1, da2,\n",
    "        input_core_dims=[[input_dims], [input_dims]],\n",
    "        output_core_dims=[[], [], [], [], [], []],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float, float, float, float, float]\n",
    "    )\n",
    "    regression_ds = xr.Dataset({\n",
    "        'slope': result[0],\n",
    "        'intercept': result[1],\n",
    "        'rvalue': result[2],\n",
    "        'pvalue': result[3],\n",
    "        'stderr': result[4],\n",
    "        'intercept_stderr': result[5]\n",
    "    })\n",
    "    # regress_da = xr.DataArray(result)\n",
    "    return regression_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bb7d42-247c-48a2-8d0d-04a42421c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the Pearson correlation and p-value statistic\n",
    "def compute_corr_pval(arr1, arr2):\n",
    "    # mask out nan and inf values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    filtered_arr1 = arr1[mask]\n",
    "    filtered_arr2 = arr2[mask]\n",
    "\n",
    "    if len(filtered_arr1) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    corr, pval = sp.stats.pearsonr(filtered_arr1, filtered_arr2)\n",
    "    return corr, pval\n",
    "\n",
    "\n",
    "# define a function to apply the ufunc to the data\n",
    "def apply_correlation(da1, da2):\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "    result = xr.apply_ufunc(\n",
    "        compute_corr_pval, da1, da2,\n",
    "        input_core_dims=[['year'], ['year']],\n",
    "        output_core_dims=[[],[]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float]\n",
    "    )\n",
    "    corr_da = result[0]\n",
    "    pval_da = result[1]\n",
    "\n",
    "    corr_ds = xr.merge([corr_da.rename('pearson_r'), pval_da.rename('p_value')])\n",
    "    return corr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14bbeadd-d740-4fb2-8496-23987dbb5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the Pearson correlation and p-value statistic\n",
    "def compute_coherence(arr1, arr2):\n",
    "    # mask out nan and inf values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    filtered_arr1 = arr1[mask]\n",
    "    filtered_arr2 = arr2[mask]\n",
    "\n",
    "    if len(filtered_arr1) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    corr, pval = sp.signal.coherence(filtered_arr1, filtered_arr2)\n",
    "    return corr, pval\n",
    "\n",
    "\n",
    "# define a function to apply the ufunc to the data\n",
    "def apply_coherence(da1, da2):\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "    result = xr.apply_ufunc(\n",
    "        compute_coherence, da1, da2,\n",
    "        input_core_dims=[['year'], ['year']],\n",
    "        output_core_dims=[[],[]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float]\n",
    "    )\n",
    "    corr_da = result[0]\n",
    "    pval_da = result[1]\n",
    "\n",
    "    corr_ds = xr.merge([corr_da.rename('pearson_r'), pval_da.rename('p_value')])\n",
    "    return corr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ede9d13-087e-48e7-8029-6c9f37c49739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the principal components of a data array\n",
    "def calc_pcs(da, **kwargs):\n",
    "\n",
    "    # # normalize da with mean and std deviation along time dimension\n",
    "    # da_mean = np.mean(da, axis=0)\n",
    "    # da_std = np.std(da, axis=0)\n",
    "    # da_norm = (da - da_mean) / da_std\n",
    "    da_norm = da\n",
    "\n",
    "    # calculate covariance matrix\n",
    "    da_cov = np.cov(da_norm, rowvar=False)\n",
    "\n",
    "    # perform eigen decomposition\n",
    "    eigenvalues, eigenvectors = sp.linalg.eigh(da_cov)\n",
    "\n",
    "    # sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # calculated principal components\n",
    "    pcs = np.dot(da_norm, eigenvectors)\n",
    "\n",
    "    return pcs, eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "# define a function to calculate the explained variance of one varialbe by another\n",
    "def calc_explained_variance(da, pcs, **kwargs):\n",
    "\n",
    "    # regress da onto pcs\n",
    "    regression = np.linalg.lstsq(pcs, da, rcond=None)[0]\n",
    "\n",
    "    # calculate explained variance by da for each PC\n",
    "    da_explained = np.dot(pcs, regression)\n",
    "\n",
    "    # calculate total variance of original pcs\n",
    "    total_variance = np.var(pcs, axis=0)\n",
    "\n",
    "    # calculate proportion of variance explained by da for each PC\n",
    "    explained_variance_ratio = np.var(da_explained, axis=0) / total_variance\n",
    "\n",
    "    return explained_variance_ratio\n",
    "\n",
    "\n",
    "# define the main function to calculate the EOF that identifies the\n",
    "# variance of da2 explained by da1\n",
    "def calc_eof(da1, da2, **kwargs):\n",
    "\n",
    "    # ensure da has dimensions (time, space), where space is (lat, lon)\n",
    "    # da1 = da1.stack(space=('latitude', 'longitude'))\n",
    "    da2_stacked = da2.stack(space=('latitude', 'longitude'))\n",
    "\n",
    "    # convert to numpy arrays for processing\n",
    "\n",
    "\n",
    "    # get pcs, eigenvalues, and eigenvectors\n",
    "    pcs, eigenvalues, eigenvectors = calc_pcs(da2_stacked)\n",
    "\n",
    "    # get explained variance ratio\n",
    "    evr = calc_explained_variance(da1, pcs)\n",
    "\n",
    "    # reshape variance ratio back to spatial dimensions\n",
    "    evr = evr.reshape((da2.sizes['latitude'], da2.sizes['longitude']))\n",
    "\n",
    "    evr_da = xr.DataArray(evr, dims=['latitude', 'longitude'],\n",
    "                          coords={'latitude': da2.coords['latitude'], 'longitude': da2.coords['longitude']})\n",
    "\n",
    "    # evr = explained_variance_ratio.unstack()\n",
    "    return evr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811a8bc-0955-4fcd-a072-54ac779c79ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mland_xr]",
   "language": "python",
   "name": "conda-env-.conda-mland_xr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
