{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c7e40f-dad0-4a24-9aa2-ce856d5e7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "# OS interaction and time\n",
    "import os\n",
    "import sys\n",
    "import cftime\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import calendar\n",
    "\n",
    "# math and data\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "from scipy.signal import detrend\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "from sklearn import linear_model\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "# random\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362c4b83-84bc-4cbf-9f06-b2fecffd4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_era5_path = '/glade/u/home/zcleveland/scratch/ERA5/'  # path to subset data\n",
    "misc_data_path = '/glade/u/home/zcleveland/scratch/misc_data/'  # path to misc data\n",
    "plot_out_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/plots/'  # path to generated plots\n",
    "scripts_main_path = '/glade/u/home/zcleveland/NAM_soil-moisture/scripts_main/'  # path to my dicts, lists, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a285032-bb1d-448f-b5e2-cd7eb1b559b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import variable lists and dictionaries\n",
    "if scripts_main_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_main_path)  # path to file containing these lists/dicts\n",
    "import my_dictionaries\n",
    "\n",
    "# my lists\n",
    "sfc_instan_list = my_dictionaries.sfc_instan_list  # instantaneous surface variables\n",
    "sfc_accumu_list = my_dictionaries.sfc_accumu_list  # accumulated surface variables\n",
    "pl_var_list = my_dictionaries.pl_var_list  # pressure level variables\n",
    "invar_var_list = my_dictionaries.invar_var_list  # invariant variables\n",
    "NAM_var_list = my_dictionaries.NAM_var_list  # NAM-based variables\n",
    "region_avg_list = my_dictionaries.region_avg_list  # region IDs for regional averages\n",
    "flux_var_list = my_dictionaries.flux_var_list  # flux variables that need to be flipped (e.g., sensible heat so that it's positive up instead of down\n",
    "misc_var_list = my_dictionaries.misc_var_list  # misc variables\n",
    "\n",
    "# my dictionaries\n",
    "var_dict = my_dictionaries.var_dict  # variables and their names\n",
    "var_units = my_dictionaries.var_units  # variable units\n",
    "region_avg_dict = my_dictionaries.region_avg_dict  # region IDs and names\n",
    "region_avg_coords = my_dictionaries.region_avg_coords  # coordinates for regions\n",
    "region_colors_dict = my_dictionaries.region_colors_dict  # colors to plot for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87def62f-a6fc-4d98-a91c-19cdfcc07642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get var files, open dataset, and subset if needed\n",
    "def get_var_data(var, region='dsw', months=[i for i in range(1,13)], **kwargs):\n",
    "    r\"\"\"\n",
    "    Retrieves the data for a given variable from my subet ERA5 dataset.  User can choose to return a dataset or data array\n",
    "    and whether to subset that data based on a region or time.  Any subset data is returned as a data array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var : str\n",
    "            The variable desired\n",
    "    region : str\n",
    "            The region desired\n",
    "    months : list, int\n",
    "            A list of months desired [1, 2, ..., 12]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    var_data : xarray Data Array\n",
    "            A data array containing the desired data, either in full or subset based on user input\n",
    "\n",
    "    Kwargs\n",
    "    ------\n",
    "    subset_flag : bool\n",
    "            True or False.  Whether to subset the data or not\n",
    "    level : int\n",
    "            The pressure level desired.  Only applied for pressure level data\n",
    "    type : str\n",
    "            Specify whether to return a dataset or data array\n",
    "    mean_flag : bool\n",
    "            True or False.  Whether to compute the mean (or sum) over the specified months\n",
    "    group_type : str\n",
    "            How to group data prior to computing mean or sum across time.\n",
    "            Options include 'year', 'month', 'dayofyear', etc.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    get_var_files : returns all files for specified variable\n",
    "    open_var_data : opens the variable dataset or data array\n",
    "    subset_var_data : subsets data array based on user input\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files = get_var_files(var, region, **kwargs)\n",
    "    var_data = open_var_data(files, var, **kwargs)\n",
    "    if kwargs.get('subset_flag', True):\n",
    "        return subset_var_data(var_data, var, months, region, **kwargs)\n",
    "    return var_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b59a3e-b868-4650-9a7d-8ddf2ff96f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the files for a given variable/region\n",
    "def get_var_files(var, region, **kwargs):\n",
    "\n",
    "    # grab files for sfc var\n",
    "    if ((var in sfc_instan_list) or (var in sfc_accumu_list)):\n",
    "        # dsw\n",
    "        if region != 'global':\n",
    "            files = glob.glob(f'{my_era5_path}dsw/*/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "        elif region == 'global':\n",
    "            files = glob.glob(f'{my_era5_path}global/*/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "    # grab files for pl var\n",
    "    elif var in pl_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}dsw/*/pl/{var.lower()}_*_dsw.nc')\n",
    "\n",
    "    # grab files for NAM var\n",
    "    elif var in NAM_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}dsw/NAM_{var}.nc')\n",
    "\n",
    "    elif var in misc_var_list:\n",
    "        files = glob.glob(f'{misc_data_path}{var}/{var}*.nc')\n",
    "\n",
    "    elif var in invar_var_list:\n",
    "        files = glob.glob(f'{my_era5_path}invariants/{var}_invariant.nc')\n",
    "\n",
    "    # if something went wrong\n",
    "    else:\n",
    "        print('something went wrong finding files')\n",
    "        files = []\n",
    "\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bbdca9-3ff8-4aaa-9240-44d9f99bf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to open variable datasets\n",
    "def open_var_data(files, var, **kwargs):\n",
    "    # get kwargs\n",
    "    var_type = kwargs.get('type', 'da')  # default to returning a data array\n",
    "\n",
    "    # open dataset\n",
    "    ds = xr.open_mfdataset(files)\n",
    "\n",
    "    # return dataset if specified\n",
    "    if type == 'ds':\n",
    "        return ds\n",
    "\n",
    "    # pull out actual variable name in the dataset since they can be different names/capitalized\n",
    "    var_name = [v for v in ds.data_vars.keys() if f'{var.upper()}' in v.upper()][0]\n",
    "    return ds[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae9994f-6b8f-4836-8cbf-364298b03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to open subset an input data set (or array) by:\n",
    "# latitude/longitude\n",
    "# time\n",
    "# averages\n",
    "def subset_var_data(var_data, var, months, region, **kwargs):\n",
    "\n",
    "    # subset to regional data if region is not DSW\n",
    "    if region in region_avg_list:\n",
    "        lats = slice(region_avg_coords[region][2], region_avg_coords[region][3])\n",
    "        lons = slice(region_avg_coords[region][0], region_avg_coords[region][1])\n",
    "    else:\n",
    "        lat_sub = kwargs.get('lat_sub', [40, 20])\n",
    "        lon_sub = kwargs.get('lon_sub', [240, 260])\n",
    "        lats = slice(lat_sub[0], lat_sub[1])\n",
    "        lons = slice(lon_sub[0], lon_sub[1])\n",
    "\n",
    "    # subset data by lat/lon\n",
    "    if 'latitude' in var_data.dims and 'longitude' in var_data.dims:\n",
    "        var_data = var_data.sel(latitude=lats, longitude=lons)\n",
    "\n",
    "    # subset to level if var is a pl var\n",
    "    if var.lower() in pl_var_list:\n",
    "        level = kwargs.get('level', None)\n",
    "        if level is not None:\n",
    "            var_data = var_data.sel(level=level)\n",
    "\n",
    "    group_type = kwargs.get('group_type', 'year')\n",
    "\n",
    "    # if var is NAM var, grouping type must be year\n",
    "    if var.lower() in NAM_var_list:\n",
    "        if ((var.lower() == 'onset') or (var.lower() == 'retreat')):\n",
    "            var_data = var_data.dt.dayofyear  # convert to dayofyear (datetime -> integer)\n",
    "        groupby_type = 'year'\n",
    "    # if var is not NAM var, subset by months\n",
    "    else:\n",
    "        # subset the data specified by months\n",
    "        var_data = var_data.sel(time=var_data['time.month'].isin(months))\n",
    "        groupby_type = f'time.{group_type}'\n",
    "\n",
    "    # subset further and compute mean/sum if specified by mean_flag\n",
    "    if region in region_avg_list:  # default to averaging regional data over lat/lon\n",
    "        dim_means = kwargs.get('dim_means', ['latitude', 'longitude'])\n",
    "    else:  # default to not averaging dsw and global data\n",
    "        dim_means = kwargs.get('dim_means', [])\n",
    "\n",
    "    time_idx_list = ['time', 'year', 'month', 'day', 'dayofyear', 'season']\n",
    "    time_idx = [v for v in time_idx_list if v in var_data.dims][0]\n",
    "    if not time_idx:\n",
    "        print('Something wrong with time index')\n",
    "        return None\n",
    "\n",
    "    return var_data.groupby(groupby_type).mean(dim=dim_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c018f335-0dfe-4c75-8365-c02f1fe9f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if inputs are list or not\n",
    "def ensure_var_list(x):\n",
    "\n",
    "    if not isinstance(x, list):\n",
    "        return [x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117b7822-643e-4eb7-8ca5-8cb0c812866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to turn a list of integers into months\n",
    "def month_num_to_name(var, months, **kwargs):\n",
    "\n",
    "    # make string for month letters from var_range (e.g. [6,7,8] -> 'JJA')\n",
    "    if var in NAM_var_list:\n",
    "        var_months = ''  # don't use months for onset, retreat, length\n",
    "    elif len(months) == 1:\n",
    "        var_months = calendar.month_name[months[0]]  # use full month name if only 1 month\n",
    "    elif len(months) == 12 and kwargs.get('mean_flag', True):\n",
    "        var_months = 'YEAR'\n",
    "    elif ((len(months) > 1) & (len(months) <= 12)):\n",
    "        var_months = ''.join([calendar.month_name[m][0] for m in months])  # make string of months, i.e. 3, 4, 5 is MAM\n",
    "    return var_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba2642c-cec4-462d-bc07-416e2cca18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to detrend the data\n",
    "\n",
    "# MANUALLY DETREND WITH LINEAR REGRESSION\n",
    "def detrend_data(arr):\n",
    "\n",
    "    # set up x array for the years\n",
    "    arr_time = np.arange(0,len(arr))\n",
    "\n",
    "    # mask out nan values\n",
    "    mask = np.isfinite(arr)\n",
    "    arr_time_mask = arr_time[mask]\n",
    "    arr_mask = arr[mask]\n",
    "\n",
    "    # make sure the array is not full of non-finite values\n",
    "    if len(arr_mask) == 0:\n",
    "        arr_detrend = np.empty(len(arr))\n",
    "        arr_detrend[:] = np.nan\n",
    "\n",
    "    else:\n",
    "        # compute linear regression\n",
    "        result = sp.stats.linregress(arr_time_mask, arr_mask)\n",
    "        m, b = result.slope, result.intercept\n",
    "\n",
    "        # detrend the data\n",
    "        arr_detrend = arr - (m*arr_time + b)\n",
    "\n",
    "    return arr_detrend\n",
    "\n",
    "\n",
    "# define a function to mask data for detrending or correlating\n",
    "def apply_detrend(da, **kwargs):\n",
    "\n",
    "    input_dims = kwargs.get('input_dims', 'time')\n",
    "    # load data\n",
    "    da.load()\n",
    "\n",
    "    da_detrend = xr.apply_ufunc(\n",
    "        detrend_data, da,\n",
    "        input_core_dims=[[input_dims]],\n",
    "        output_core_dims=[[input_dims]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[da.dtype]\n",
    "    )\n",
    "\n",
    "    return da_detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a613208-7a01-44b4-8fbb-4901294445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to regress data\n",
    "def regress_data(arr1, arr2):\n",
    "\n",
    "    # mask out nan values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    arr1_mask = arr1[mask]\n",
    "    arr2_mask = arr2[mask]\n",
    "\n",
    "    if len(arr1_mask) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    else:\n",
    "        # compute linear regression\n",
    "        res = sp.stats.linregress(arr1_mask, arr2_mask)\n",
    "        return res.slope, res.intercept, res.rvalue, res.pvalue, res.stderr, res.intercept_stderr\n",
    "\n",
    "\n",
    "# define a function to mask data for detrending or correlating\n",
    "def apply_regression(da1, da2, **kwargs):\n",
    "\n",
    "    input_dims = kwargs.get('input_dims', 'time')\n",
    "    # load data\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "\n",
    "    result = xr.apply_ufunc(\n",
    "        regress_data, da1, da2,\n",
    "        input_core_dims=[[input_dims], [input_dims]],\n",
    "        output_core_dims=[[], [], [], [], [], []],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float, float, float, float, float]\n",
    "    )\n",
    "    regression_ds = xr.Dataset({\n",
    "        'slope': result[0],\n",
    "        'intercept': result[1],\n",
    "        'rvalue': result[2],\n",
    "        'pvalue': result[3],\n",
    "        'stderr': result[4],\n",
    "        'intercept_stderr': result[5]\n",
    "    })\n",
    "    # regress_da = xr.DataArray(result)\n",
    "    return regression_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bb7d42-247c-48a2-8d0d-04a42421c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the Pearson correlation and p-value statistic\n",
    "def compute_corr_pval(arr1, arr2, **kwargs):\n",
    "    # mask out nan and inf values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    filtered_arr1 = arr1[mask]\n",
    "    filtered_arr2 = arr2[mask]\n",
    "\n",
    "    if len(filtered_arr1) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    corr, pval = sp.stats.pearsonr(filtered_arr1, filtered_arr2)\n",
    "    return corr, pval\n",
    "\n",
    "\n",
    "# define a function to apply the ufunc to the data\n",
    "def apply_correlation(da1, da2, **kwargs):\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "    input_dims = kwargs.get('input_dims', 'year')\n",
    "    result = xr.apply_ufunc(\n",
    "        compute_corr_pval, da1, da2,\n",
    "        input_core_dims=[[input_dims], [input_dims]],\n",
    "        output_core_dims=[[],[]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float]\n",
    "    )\n",
    "    corr_da = result[0]\n",
    "    pval_da = result[1]\n",
    "\n",
    "    corr_ds = xr.merge([corr_da.rename('pearson_r'), pval_da.rename('p_value')])\n",
    "    return corr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bbeadd-d740-4fb2-8496-23987dbb5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the Pearson correlation and p-value statistic\n",
    "def compute_coherence(arr1, arr2):\n",
    "    # mask out nan and inf values\n",
    "    mask = np.isfinite(arr1) & np.isfinite(arr2)\n",
    "    filtered_arr1 = arr1[mask]\n",
    "    filtered_arr2 = arr2[mask]\n",
    "\n",
    "    if len(filtered_arr1) < 2:  # check if there are enough data points\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    corr, pval = sp.signal.coherence(filtered_arr1, filtered_arr2)\n",
    "    return corr, pval\n",
    "\n",
    "\n",
    "# define a function to apply the ufunc to the data\n",
    "def apply_coherence(da1, da2):\n",
    "    da1.load()\n",
    "    da2.load()\n",
    "    result = xr.apply_ufunc(\n",
    "        compute_coherence, da1, da2,\n",
    "        input_core_dims=[['year'], ['year']],\n",
    "        output_core_dims=[[],[]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float, float]\n",
    "    )\n",
    "    corr_da = result[0]\n",
    "    pval_da = result[1]\n",
    "\n",
    "    corr_ds = xr.merge([corr_da.rename('pearson_r'), pval_da.rename('p_value')])\n",
    "    return corr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ede9d13-087e-48e7-8029-6c9f37c49739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the principal components of a data array\n",
    "def calc_pcs(da, **kwargs):\n",
    "\n",
    "    # # normalize da with mean and std deviation along time dimension\n",
    "    # da_mean = np.mean(da, axis=0)\n",
    "    # da_std = np.std(da, axis=0)\n",
    "    # da_norm = (da - da_mean) / da_std\n",
    "    da_norm = da\n",
    "\n",
    "    # calculate covariance matrix\n",
    "    da_cov = np.cov(da_norm, rowvar=False)\n",
    "\n",
    "    # perform eigen decomposition\n",
    "    eigenvalues, eigenvectors = sp.linalg.eigh(da_cov)\n",
    "\n",
    "    # sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # calculated principal components\n",
    "    pcs = np.dot(da_norm, eigenvectors)\n",
    "\n",
    "    return pcs, eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "# define a function to calculate the explained variance of one varialbe by another\n",
    "def calc_explained_variance(da, pcs, **kwargs):\n",
    "\n",
    "    # regress da onto pcs\n",
    "    regression = np.linalg.lstsq(pcs, da, rcond=None)[0]\n",
    "\n",
    "    # calculate explained variance by da for each PC\n",
    "    da_explained = np.dot(pcs, regression)\n",
    "\n",
    "    # calculate total variance of original pcs\n",
    "    total_variance = np.var(pcs, axis=0)\n",
    "\n",
    "    # calculate proportion of variance explained by da for each PC\n",
    "    explained_variance_ratio = np.var(da_explained, axis=0) / total_variance\n",
    "\n",
    "    return explained_variance_ratio\n",
    "\n",
    "\n",
    "# define the main function to calculate the EOF that identifies the\n",
    "# variance of da2 explained by da1\n",
    "def calc_eof(da1, da2, **kwargs):\n",
    "\n",
    "    # ensure da has dimensions (time, space), where space is (lat, lon)\n",
    "    # da1 = da1.stack(space=('latitude', 'longitude'))\n",
    "    da2_stacked = da2.stack(space=('latitude', 'longitude'))\n",
    "\n",
    "    # convert to numpy arrays for processing\n",
    "\n",
    "\n",
    "    # get pcs, eigenvalues, and eigenvectors\n",
    "    pcs, eigenvalues, eigenvectors = calc_pcs(da2_stacked)\n",
    "\n",
    "    # get explained variance ratio\n",
    "    evr = calc_explained_variance(da1, pcs)\n",
    "\n",
    "    # reshape variance ratio back to spatial dimensions\n",
    "    evr = evr.reshape((da2.sizes['latitude'], da2.sizes['longitude']))\n",
    "\n",
    "    evr_da = xr.DataArray(evr, dims=['latitude', 'longitude'],\n",
    "                          coords={'latitude': da2.coords['latitude'], 'longitude': da2.coords['longitude']})\n",
    "\n",
    "    # evr = explained_variance_ratio.unstack()\n",
    "    return evr_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8875843-dd80-4c0e-aa36-7bc6d326bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to zscore a variable\n",
    "def calc_zscore_monthly(da, **kwargs):\n",
    "    # assuming da is input with dimensions (year: , month: , ...)\n",
    "    da_mean = da.mean(dim='year')\n",
    "    da_std = da.std(dim='year')\n",
    "    da_zscore = (da - da_mean) / da_std\n",
    "\n",
    "    return da_zscore\n",
    "\n",
    "\n",
    "# define a function to apply the calc_zscore function to an xarray dataset\n",
    "def apply_zscore(da, **kwargs):\n",
    "    pass\n",
    "    # convert time index to year and month indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811a8bc-0955-4fcd-a072-54ac779c79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "\n",
    "v1 = 'swvl1'\n",
    "v1_months = [3, 4, 5]\n",
    "v2 = 'onset'\n",
    "v2_months = [6, 7, 8]\n",
    "\n",
    "var1 = get_var_data(v1, 'cp')\n",
    "var2 = get_var_data(v2)\n",
    "\n",
    "# detrend the data\n",
    "var1_detrend = apply_detrend(var1, input_dims='year')\n",
    "var2_detrend = apply_detrend(var2, input_dims='year')\n",
    "\n",
    "# compute monthly means\n",
    "var1_detrend_monthly = time_to_year_month_avg(var1_detrend)\n",
    "var2_detrend_monthly = time_to_year_month_avg(var2_detrend)\n",
    "\n",
    "# compute zscores\n",
    "var1_zscore = calc_zscore_monthly(var1_detrend_monthly)\n",
    "var2_zscore = calc_zscore_monthly(var2_detrend_monthly)\n",
    "\n",
    "# compute correlation and linear regression of summer z onto spring sd\n",
    "var1_spring = var1_zscore.sel(month=v1_months).mean(dim='month')\n",
    "var2_summer = var2_zscore.sel(month=v2_months).mean(dim='month')\n",
    "corr_ds = apply_correlation(var1_spring, var2_summer, input_dims='year')\n",
    "regression_ds = apply_regression(var1_spring, var2_summer, input_dims='year')\n",
    "\n",
    "\n",
    "# plot correlation and pvalues\n",
    "corr_da = corr_ds['pearson_r']\n",
    "pval_da = corr_ds['p_value']\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(projection=projection))\n",
    "\n",
    "# create contour levels and hatches for plotting\n",
    "corr_levels = np.arange(-1, 1.05, 0.05)\n",
    "\n",
    "# plot the data using contourf\n",
    "corr_cf = plt.contourf(corr_da.longitude, corr_da.latitude,\n",
    "                       corr_da, levels=corr_levels,\n",
    "                       cmap='RdBu_r', extend='both')\n",
    "\n",
    "# extract coordinates where p-value < 0.1 (dots) and p-value < 0.05 (triangles)\n",
    "lat, lon = np.meshgrid(pval_da.latitude, pval_da.longitude, indexing='ij')\n",
    "mask_dots = (pval_da <= 0.1) & (pval_da >= 0.05)\n",
    "mask_triangles = pval_da <= 0.05\n",
    "\n",
    "# Plot dots (p-value < 0.1 and >= 0.05)\n",
    "plt.scatter(lon[mask_dots], lat[mask_dots], color='black', marker='.',\n",
    "            s=5, transform=ccrs.PlateCarree(), label='0.05 <= p < 0.1')\n",
    "\n",
    "# Plot triangles (p-value < 0.05)\n",
    "plt.scatter(lon[mask_triangles], lat[mask_triangles], color='black', marker='^',\n",
    "            s=8, transform=ccrs.PlateCarree(), label='p < 0.05')\n",
    "\n",
    "# add coastlines, state borders, and other features\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "\n",
    "plt.colorbar(corr_cf, ax=ax, label=f'pearson r', pad=0.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# plot regression data\n",
    "var1_min = var1_spring.min(dim=['year'])\n",
    "var1_max = var2_summer.max(dim=['year'])\n",
    "var2_min = regression_ds['slope'] * var1_min + regression_ds['intercept']\n",
    "var2_max = regression_ds['slope'] * var1_max + regression_ds['intercept']\n",
    "var2_spread = var2_max - var2_min\n",
    "\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection=projection))\n",
    "\n",
    "# create contour levels and hatches for plotting\n",
    "vmin = np.nanmin(var2_spread)\n",
    "vmax = np.nanmax(var2_spread)\n",
    "cf_levels = np.linspace(vmin, vmax, 50)\n",
    "\n",
    "if vmin < 0 and vmax > 0:\n",
    "    norm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "    cmap = 'RdBu_r'\n",
    "else:\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = 'Blues' if vmax <= 0 else 'Reds'\n",
    "\n",
    "# plot the data using contourf\n",
    "regress_cf = plt.contourf(var2_spread.longitude, var2_spread.latitude,\n",
    "                          var2_spread, levels=cf_levels,\n",
    "                          cmap=cmap, norm=norm, extend='both')\n",
    "\n",
    "regress_cs = plt.contour(regression_ds.longitude, regression_ds.latitude,\n",
    "                         regression_ds['slope'], levels=10, linewidths=0.5, linestyles='--', colors='black')\n",
    "\n",
    "# add coastlines, state borders, and other features\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "\n",
    "plt.colorbar(regress_cf, ax=ax, label=f'm', pad=0.02)\n",
    "plt.clabel(regress_cs, inline=True, fontsize=8, fmt='%1.1f')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb83603-a8d3-4b39-a65b-dd8f249bafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_year_dayofyear(ds):\n",
    "    year = ds.time.dt.year\n",
    "    dayofyear = ds.time.dt.dayofyear\n",
    "\n",
    "    # Assign new coordinates\n",
    "    ds = ds.assign_coords(year=(\"time\", year.data), dayofyear=(\"time\", dayofyear.data))\n",
    "\n",
    "    # Set the index to year and day and unstack\n",
    "    return ds.set_index(time=(\"year\", \"dayofyear\")).unstack(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a84f634-66b4-48ed-af2a-5f31f00f4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_year_month_avg(ds):\n",
    "    years = ds.groupby('time.year').mean('time').year\n",
    "    months = ds.groupby('time.month').mean('time').month\n",
    "\n",
    "    # make a pandas MultiIndex that is years x months\n",
    "    midx = pd.MultiIndex.from_product([years.values,months.values], names=(\"year\",\"month\"))\n",
    "\n",
    "    ds_temp = ds.resample(time='1M').mean(dim='time')\n",
    "\n",
    "    # try to multiindex the dataset directly\n",
    "    ds_temp = ds_temp.assign_coords({'time':midx})\n",
    "\n",
    "    ds_out = ds_temp.unstack()\n",
    "\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb703c72-a441-4c77-986f-d266297ed712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_leap_days(ds):\n",
    "    # convert time dimension to pandas\n",
    "    time_index = pd.DatetimeIndex(ds['time'].values)\n",
    "\n",
    "    # identify dates that are feb 29 on non leap years\n",
    "    feb_29_non_leap_years = time_index[(time_index.month == 2) & (time_index.day == 29) & (~time_index.is_leap_year)]\n",
    "\n",
    "    # filter out feb 29 on non leap years\n",
    "    filtered_time_index = time_index.difference(feb_29_non_leap_years)\n",
    "\n",
    "    # index data array to exclude feb 29 on non leap years\n",
    "    filtered_ds = ds.sel(time=filtered_time_index)\n",
    "    return filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf14ee8e-aff7-49d3-aa5f-c1c369382edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_dayofyear_to_time(ds):\n",
    "    # Stack year and day back into a multiindex time\n",
    "    ds_stacked = ds.stack(time=('year', 'dayofyear'))\n",
    "\n",
    "    # Convert the multiindex to datetime\n",
    "    time_index = pd.to_datetime(ds_stacked.indexes['time'].to_frame().apply(lambda x: f'{x[0]}-{x[1]}', axis=1), format='%Y-%j')\n",
    "\n",
    "    # Create a new DataArray with the correct time index\n",
    "    ds_stacked = ds_stacked.assign_coords({'time':time_index.values})\n",
    "    ds_stacked = ds_stacked.swap_dims({'time': 'time'})\n",
    "\n",
    "    # Drop the old coordinates year and day\n",
    "    ds_stacked = ds_stacked.reset_coords(['year', 'dayofyear'], drop=True)\n",
    "\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2975a9-79a8-47d0-9089-300d101dd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_day_to_time(ds):\n",
    "    # Stack year and day back into a multiindex time\n",
    "    ds_stacked = ds.stack(time=('year', 'day'))\n",
    "\n",
    "    # Convert the multiindex to datetime\n",
    "    time_index = pd.to_datetime(ds_stacked.indexes['time'].to_frame().apply(lambda x: f'{x[0]}-{x[1]}', axis=1), format='%Y-%j')\n",
    "\n",
    "    # Create a new DataArray or Dataset with the correct time index and original data\n",
    "    new_ds = ds_stacked.assign_coords(time=time_index).swap_dims({'time': 'time'}).reset_coords(['year', 'day'], drop=True)\n",
    "\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e6a484-80bd-4984-b631-3f2969af22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_split_time(ds, dim_to_split, new_dim1, new_dim2):\n",
    "    # get the values for the new dimensions\n",
    "    dim1 = getattr(ds[dim_to_split].dt, new_dim1)\n",
    "    dim2 = getattr(ds[dim_to_split].dt, new_dim2)\n",
    "\n",
    "    # assign new coordinates dynamically\n",
    "    ds = ds.assign_coords(**{new_dim1: (dim_to_split, dim1.data), new_dim2: (dim_to_split, dim2.data)})\n",
    "\n",
    "    # set the index to the new dimensions\n",
    "    ds = ds.set_index({dim_to_split: (new_dim1, new_dim2)})\n",
    "\n",
    "    # unstack the new dimensions to reshape the array\n",
    "    ds = ds.unstack(dim_to_split)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed8b5a18-809e-4a68-a7bb-3de22ba869f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_split_time(ds, dim_to_split, new_dim1, new_dim2):\n",
    "    # Get the values for the new dimensions\n",
    "    dim1 = getattr(ds[dim_to_split].dt, new_dim1)\n",
    "    dim2 = getattr(ds[dim_to_split].dt, new_dim2)\n",
    "\n",
    "    # Ensure uniqueness by including the day of the month\n",
    "    days = ds[dim_to_split].dt.day\n",
    "\n",
    "    # Assign new coordinates dynamically\n",
    "    ds = ds.assign_coords(\n",
    "        **{new_dim1: (dim_to_split, dim1.data), \n",
    "           new_dim2: (dim_to_split, dim2.data),\n",
    "           'day': (dim_to_split, days.data)}\n",
    "    )\n",
    "\n",
    "    # Set the index to the new dimensions\n",
    "    ds = ds.set_index({dim_to_split: (new_dim1, new_dim2, 'day')})\n",
    "\n",
    "    # Unstack the new dimensions to reshape the array\n",
    "    ds = ds.unstack(dim_to_split)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cffe1b-4478-4709-b3af-a89614907bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_reshaped = time_to_year_day(sd)\n",
    "sd_mean = sd_reshaped.mean(dim='year')\n",
    "sd_std = sd_reshaped.std(dim='year')\n",
    "sd_norm = (sd_reshaped - sd_mean) / sd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373390ad-a681-4ed5-88e0-d8d7b2cdee53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mland_xr]",
   "language": "python",
   "name": "conda-env-.conda-mland_xr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
