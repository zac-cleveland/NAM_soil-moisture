{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e158896-bb9b-4409-9135-53a5fc849103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "# OS interaction and time\n",
    "import os\n",
    "import sys\n",
    "import cftime\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import calendar\n",
    "\n",
    "# math and data\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "from sklearn import linear_model\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "# random\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "629946cd-81f1-432d-b51f-1ae716d16f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = '/glade/campaign/collections/rda/data/ds633.0/'  # base path to ERA5 data on derecho\n",
    "out_path = '/glade/u/home/zcleveland/scratch/ERA5/'  # base path to my subsetted data\n",
    "era5_invariant_path = '/glade/campaign/collections/rda/data/ds633.0/e5.oper.invariant/197901/'  # path to ERA5 directory for invariant data\n",
    "my_invariant_path = '/glade/u/home/zcleveland/scratch/ERA5/invariants/'  # path to my invariant directory\n",
    "era5_pl_path = '/glade/campaign/collections/rda/data/ds633.0/e5.oper.an.pl/'  # path to ERA5 pressure level data\n",
    "sub_script_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/scripts/subsetting/'  # path to my subsetting scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e6a39-9db8-4670-af88-ad2b00afac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable list to choose\n",
    "var_list = [\n",
    "    # 'lsp',  # large scale precipitation (m of water) - accumu\n",
    "    # 'cp',  # convective precipitation (m of water) - accumu\n",
    "    # 'tp',  # total precipitation (m of water) - accumu -- DERIVED\n",
    "    # 'sd',  # snow depth  (m of water equivalent) - instan\n",
    "    # 'msl',  # mean sea level pressure (Pa) - instan\n",
    "    # 'tcc',  # total cloud cover (0-1) - instan\n",
    "    # 'stl1',  # soil temp layer 1 (K) - instan\n",
    "    # 'stl2',  # soil temp layer 2 (K) - instan\n",
    "    # 'stl3',  # soil temp layer 3 (K) - instan\n",
    "    # 'stl4',  # soil temp layer 4 (K) - instan\n",
    "    # 'swvl1',  # soil volume water content layer 1 (m^3 m^-3) - instan\n",
    "    # 'swvl2',  # soil volume water content layer 2 (m^3 m^-3) - instan\n",
    "    # 'swvl3',  # soil volume water content layer 3 (m^3 m^-3) - instan\n",
    "    # 'swvl4',  # soil volume water content layer 4 (m^3 m^-3) - instan\n",
    "    # '2t',  # 2 meter temp (K) - instan\n",
    "    # '2d',  # 2 meter dew point (K) - instan\n",
    "    # 'ishf',  # instant surface heat flux (W m^-2) - instan\n",
    "    # 'ie',  # instant moisture flux (kg m^-2 s^-1) - instan\n",
    "    # 'sshf',  # surface sensible heat flux (J m^-2) - accumu\n",
    "    # 'slhf',  # surface latent heat flux (J m^-2) - accumu\n",
    "    # 'ssr',  # surface net solar radiation (J m^-2) - accumu\n",
    "    # 'str',  # surface net thermal radiation (J m^-2) - accumu\n",
    "    # 'sro',  # surface runoff (m) - accumu\n",
    "    # 'sf',  # total snowfall (m of water equivalent) - accumu\n",
    "    # 'cape',  # convective available potential energy (J kg^-1) - instan\n",
    "    # 'tcw',  # total column water (kg m^-2) - sfc (sum total of solid, liquid, and vapor in a column)\n",
    "    # 'ssrd',  # surface solar radiation downwards (J m^-2) - accumu\n",
    "    # 'strd',  # surface thermal radiation downwards (J m^-2) - accumu\n",
    "    # 'ttr',  # top net thermal radiation (OLR, J m^-2) - accumu -- divide by time (s) for W m^-2\n",
    "    'sstk',  # sea surface temperature (K) - instan\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426c0606-e90d-4c22-851a-ed87359788de",
   "metadata": {},
   "outputs": [],
   "source": [
    "invariant_list = [\n",
    "    # 'cl',  # lake cover (0-1)\n",
    "    # 'dl',  # lake depth (m)\n",
    "    # 'cvl',  # low vegetation cover (0-1)\n",
    "    # 'cvh',  # high vegetation cover (0-1)\n",
    "    # 'tvl',  # type of low vegetation (~)\n",
    "    # 'tvh',  # type of high vegetation (~)\n",
    "    # 'slt',  # soil type1 (~)\n",
    "    # 'sdfor',  # standard deviation of filtered subgrid orography (m)\n",
    "    # 'z',  # geopotential (m^2 s^-2)\n",
    "    # 'sdor',  # standard deviation of orography (~)\n",
    "    # 'isor',  # anisotropy of sub-gridscale orography (~)\n",
    "    # 'anor',  # angle of sub-gridscale orography (radians)\n",
    "    # 'slor',  # slope of sub-gridscale orography (~)\n",
    "    # 'lsm',  # land-sea mask (0-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d78fb1a-a66a-42e2-9ca9-fd1eb150f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_var_list = [\n",
    "    # 'pv',  # potential vorticity (K m^2 kg^-1 s^-1)\n",
    "    # 'crwc',  # specific rain water content (kg kg^-1)\n",
    "    # 'cswc',  # specific snow water content (kg kg^-1)\n",
    "    # 'z',  # geopotential (m^2 s^2)\n",
    "    # 't',  # temperature (K)\n",
    "    # 'u',  # u component of wind(m s^-1)\n",
    "    # 'v',  # v component of wind (m s^-1)\n",
    "    # 'q',  # specific humidity (kg kg^-1)\n",
    "    # 'w',  # vertical velo|city (Pa s^-1)\n",
    "    # 'vo',  # vorticity - relative (s^-1)\n",
    "    # 'd',  # divergence (s^-1)\n",
    "    # 'r',  # relative humidity (%)\n",
    "    # 'clwc',  # specific cloud liquid water content\n",
    "    # 'ciwc',  # specific cloud ice water content\n",
    "    # 'cc',  # fraction of cloud cover (0-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4642ee-e180-48fa-93ce-9fd44476396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract, subset, and process data for single variable in the desert southwest\n",
    "def dsw_subset_era5(variable='lsp', start_date=200101, end_date=200102, dsw_flag=True):\n",
    "    print(f'. . . Processing variable: {variable}: {start_date}_{end_date} . . .\\n')\n",
    "\n",
    "    # exit if not yearly data\n",
    "    if (end_date-start_date)>11:\n",
    "        print(f'Time range greater than 1 year. Skipping... ')\n",
    "        print(f'start_date: {start_date}\\n end_date: {end_date}\\n')\n",
    "        return\n",
    "\n",
    "    start_time = time.time()  # keep track of time to process.\n",
    "    # split start and end date to get year and month\n",
    "    start_year, start_month = f'{start_date}'[:4], f'{start_date}'[4:]\n",
    "    end_year, end_month = f'{end_date}'[:4], f'{end_date}'[4:]\n",
    "\n",
    "    # define output filename and path\n",
    "    if dsw_flag:\n",
    "        out_region = 'dsw'\n",
    "    else:\n",
    "        out_region = 'global'\n",
    "\n",
    "    out_fn = f'{variable}_{start_date}_{end_date}_dsw'  # out file name\n",
    "    out_fp = f'{out_path}{out_region}/{start_year}/{out_fn}'  # out file path (including file name)\n",
    "\n",
    "    # check if file already exists\n",
    "    if (os.path.exists(f'{out_fp}.nc') or\n",
    "        os.path.exists(f'{out_fp}_min.nc') or\n",
    "        os.path.exists(f'{out_fp}_max.nc') or\n",
    "        os.path.exists(f'{out_fp}_avg.nc')):\n",
    "\n",
    "        print(f'File {out_fn} already exists. Skipping...\\n')\n",
    "        return\n",
    "\n",
    "    # find the directory that the variable exists in\n",
    "    print(f'Searching for {variable} in {era5_path}\\n')\n",
    "    contents = os.listdir(era5_path)  # all contents in era5_path\n",
    "    # only get directories from era5_path\n",
    "    directories = [item for item in contents if os.path.isdir(os.path.join(era5_path, item))]\n",
    "    found = []  # initialize loop exit condition\n",
    "    for dir in directories:\n",
    "        files = os.listdir(f'{era5_path}{dir}/{197901}')\n",
    "        for file in files:\n",
    "            if f'_{variable}.' in file:  # check for existence of the var key in the file names\n",
    "                print(f'{variable} found at {dir}\\n')\n",
    "                found.append(1)\n",
    "                var_dir = dir\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    if not found:\n",
    "        print(f'No files found with {variable}. Skipping...\\n')\n",
    "        return\n",
    "\n",
    "    # find files for the variable in the specified directory\n",
    "    files = []\n",
    "    for year in range(int(start_year), int(end_year)+1):  # input should be same year, so add 1 to end for \"range\" function\n",
    "        for month in range(1, 13):  # months 1-12 (jan-dec)\n",
    "            if month < 10:  # add a '0' to match date string format\n",
    "                year_month = f'{year}0{month}'\n",
    "            else:\n",
    "                year_month = f'{year}{month}'\n",
    "            try:\n",
    "                if ((f'{year_month}' < f'{start_date}') or (f'{year_month}' > f'{end_date}')):\n",
    "                    pass  # in case the date is outside the range, just pass it\n",
    "                else:\n",
    "                    files += glob.glob(f'{era5_path}/{var_dir}/{year_month}/*_{variable}.*.nc', recursive=True)\n",
    "            except Exception as e:\n",
    "                print(f'Error in {era5_path}/{var_dir}/{year}0{month}/*_{variable}.*.nc: {e}\\n')\n",
    "    files.sort()\n",
    "\n",
    "    # calculate total number of directories for sanity check\n",
    "    total_directories = len(files)\n",
    "    print(f'{total_directories} number of files\\n')\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "\n",
    "        # open datasets\n",
    "        print(f'opening datasets')\n",
    "        ds = xr.open_mfdataset(files)\n",
    "\n",
    "        if dsw_flag:\n",
    "            # subset the data for the Desert Southwest\n",
    "            lat_range = slice(40, 20)  # lat range 20 N to 40 N\n",
    "            lon_range = slice(240, 260)  # lon range 240 (120 W) to 260 (100 W)\n",
    "            ds_sub = ds.sel(latitude=lat_range, longitude=lon_range, drop=True)\n",
    "        else:\n",
    "            ds_sub = ds\n",
    "\n",
    "        # dimensions of accumulation data, instant data, etc.\n",
    "        acc_dims = ['forecast_hour', 'forecast_initial_time', 'latitude', 'longitude']\n",
    "        inst_dims = ['latitude', 'longitude', 'time']\n",
    "        pl_dims = ['latitude', 'level', 'longitude', 'time']\n",
    "\n",
    "        ### FOR ACCUMULCATIONS ###\n",
    "        if set(ds_sub.dims) == set(acc_dims):\n",
    "            # calculate sum total\n",
    "            daily_data = ds_sub.sum(dim='forecast_hour').resample(forecast_initial_time='1D').sum()\n",
    "            daily_data = daily_data.rename({'forecast_initial_time': 'time'}) # rename time dimension\n",
    "\n",
    "        ### FOR DAILY AVERAGE, MIN, AND MAX ###\n",
    "        elif set(ds_sub.dims) == set(inst_dims):\n",
    "\n",
    "            temp = ds_sub.resample(time='1D')  # turn into daily data\n",
    "\n",
    "            # find average and rename the variable to include AVG\n",
    "            daily_avg = temp.mean(dim='time')\n",
    "            var_xx = [varx for varx in daily_avg.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_avg = daily_avg.rename_vars({f'{var_xx}': f'{var_xx}_AVG'})\n",
    "\n",
    "            # find maximum and rename the variable to include MAX\n",
    "            daily_max = temp.max(dim='time')\n",
    "            var_xx = [varx for varx in daily_max.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_max = daily_max.rename_vars({f'{var_xx}': f'{var_xx}_MAX'})\n",
    "\n",
    "            # find minimum and rename the variable to include MIN\n",
    "            daily_min = temp.min(dim='time')\n",
    "            var_xx = [varx for varx in daily_min.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_min = daily_min.rename_vars({f'{var_xx}': f'{var_xx}_MIN'})\n",
    "\n",
    "            # combining data\n",
    "            print(f'Combining data\\n')\n",
    "            daily_data = xr.merge([daily_avg,daily_max,daily_min], compat='override')\n",
    "\n",
    "        ### FOR PRESSURE LEVELS ###\n",
    "        elif set(ds_sub.dims) == set(pl_dims):\n",
    "\n",
    "            # re-subset to only grab certain levels\n",
    "            pl_levels = [1000, 900, 800, 700, 600, 500, 400, 300]\n",
    "            ds_sub = ds_sub.sel(level=pl_levels, drop=True)\n",
    "\n",
    "            temp = ds_sub.resample(time='1D')  # turn into daily data\n",
    "\n",
    "            # find average and rename the variable to include AVG\n",
    "            daily_avg = temp.mean(dim='time')\n",
    "            var_xx = [varx for varx in daily_avg.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_data = daily_avg.rename_vars({f'{var_xx}': f'{var_xx}_AVG'})\n",
    "\n",
    "        else:\n",
    "            print(F'Dimensional error finding daily values for {variable}')\n",
    "            print(f'Dimensions are {sorted(ds_sub.dims)}.\\n Skipping...\\n')\n",
    "            return\n",
    "\n",
    "        # write data to NetCDF file\n",
    "        print(f'Writing data to NetCDF\\n')\n",
    "        daily_data.to_netcdf(f'{out_fp}.nc')\n",
    "\n",
    "    print(f'\\rTime elapsed: {time.time()-start_time: .2f} s\\n')\n",
    "\n",
    "    # return ds, ds_sub, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdbe023d-74c7-4058-98bb-b23928a0e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time array to loop through\n",
    "years = np.arange(1980,2020)\n",
    "months = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cb813-938c-427f-a477-0a0ace606405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through variables in var_list and process each one for DSW\n",
    "for var in var_list:\n",
    "    for year in years:\n",
    "        with open(f'{sub_script_path}pl_out.txt', 'a') as file:\n",
    "            file.write(f'{var} - {year} - DSW\\n')\n",
    "        start_date = int(f'{year}01')\n",
    "        end_date = int(f'{year}12')\n",
    "        dsw_subset_era5(var, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfae8e0-57a0-4d9e-af5d-daa031d38817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through variables in var_list and process each one for GLOBAL\n",
    "for var in var_list:\n",
    "    for year in years:\n",
    "        with open(f'{sub_script_path}pl_out.txt', 'a') as file:\n",
    "            file.write(f'{var} - {year} - GLOBAL\\n')\n",
    "        start_date = int(f'{year}01')\n",
    "        end_date = int(f'{year}12')\n",
    "        dsw_subset_era5(var, start_date, end_date, dsw_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43264879-0774-489f-9259-8caa1f0c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to subset the invariant data from ERA5\n",
    "def invariant_subset_era5(var='lsm'):\n",
    "\n",
    "    # open dataset\n",
    "    var_file = glob.glob(f'{era5_invariant_path}e5.oper.invariant.*_{var}*.nc')\n",
    "    ds = xr.open_mfdataset(var_file)\n",
    "    da = ds.isel(time=0)[f'{var.upper()}']\n",
    "\n",
    "    # save invariant to netcdf file in local directory\n",
    "    da.to_netcdf(f'{my_invariant_path}{var}_invariant.nc')\n",
    "\n",
    "    # check if var = geopotential and if so, create elevation variable\n",
    "    if var == 'z':\n",
    "        elevation = da / 9.80665\n",
    "        elevation.rename('ELEVATION')\n",
    "        # save elevation to netcdf\n",
    "        elevation.to_netcdf(f'{my_invariant_path}elevation_invariant.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dba0019a-7208-4e2e-b1db-0bb5ba5faa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in invariant_list:\n",
    "#     invariant_subset_era5(var=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bce08f5-9935-4753-8884-f900a40e2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to subset pressure level data from ERA5\n",
    "def pl_subset_era5(variable='pv', start_date = 200101):\n",
    "    print(f'. . . Processing variable: {variable}: {start_date} . . .\\n')\n",
    "\n",
    "    start_time = time.time()  # keep track of time to process.\n",
    "    # split start and end date to get year and month\n",
    "    start_year = f'{start_date}'[:4]\n",
    "    # define output filename and path\n",
    "    out_fn = f'{variable}_{start_date}_dsw'  # out file name\n",
    "    out_fp = f'{out_path}{start_year}/pl/{out_fn}'  # out file path (including file name)\n",
    "\n",
    "    # check if file already exists\n",
    "    if (os.path.exists(f'{out_fp}.nc')):\n",
    "        print(f'File {out_fn} already exists. Skipping...\\n')\n",
    "        return\n",
    "\n",
    "    # grab files for the variable and dates\n",
    "    files = glob.glob(f'{era5_path}e5.oper.an.pl/{start_date}/*_{variable}.*.nc')\n",
    "    files.sort()\n",
    "\n",
    "    lat_range = slice(40, 20)  # lat range 20 N to 40 N\n",
    "    lon_range = slice(240, 260)  # lon range 240 (120 W) to 260 (100 W)\n",
    "    pl_levels = [1000, 900, 800, 700, 600, 500, 400, 300]\n",
    "\n",
    "    ds_list = []\n",
    "    for i in range(len(files)):\n",
    "\n",
    "        # open datasets\n",
    "        ds = xr.open_mfdataset(files[i], data_vars='minimal', coords='minimal', parallel=True, chunks={'time': 1})\n",
    "\n",
    "        # subset the data for the Desert Southwest\n",
    "        ds_sub = ds.sel(time=ds.time[0], level=pl_levels, latitude=lat_range, longitude=lon_range)\n",
    "        ds_sub.load()\n",
    "\n",
    "        ds_list.append(ds_sub)\n",
    "\n",
    "    daily_data = xr.concat(ds_list, dim='time')\n",
    "\n",
    "    # write data to NetCDF file\n",
    "    print(f'Writing data to NetCDF\\n')\n",
    "    daily_data.to_netcdf(f'{out_fp}.nc')\n",
    "    print(f'\\rTime elapsed: {time.time()-start_time: .2f} s\\n')\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f78efa-9832-4207-ab77-393a390ef1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time array to loop through\n",
    "# years = np.arange(1980,2020)\n",
    "# months = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211d51c-d390-41f3-bf5f-7af5643875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through variables in pl_var_list and process each one\n",
    "# for var in pl_var_list:\n",
    "#     with open(f'{sub_script_path}pl_out.txt', 'a') as file:\n",
    "#         file.write(f'\\n. . . . . . . . . . \\n{var}\\n. . . . . . . . . .\\n')\n",
    "#     for year in years:\n",
    "#         with open(f'{sub_script_path}pl_out.txt', 'a') as file:\n",
    "#             file.write(f'{var} - {year}\\n')\n",
    "#         # month_list = []\n",
    "#         for month in months:\n",
    "#             print(month)\n",
    "#             with open(f'{sub_script_path}pl_out.txt', 'a') as file:\n",
    "#                 file.write(f'{month} . ')\n",
    "#             if month<10:\n",
    "#                 start_date = int(f'{year}0{month}')\n",
    "#             elif month>=10:\n",
    "#                 start_date = int(f'{year}{month}')\n",
    "\n",
    "#             monthly_data = pl_subset_era5(var, start_date)\n",
    "\n",
    "            # month_list.append(monthly_data)\n",
    "\n",
    "        # year_fn = f'{var}_{year}01_{year}12_dsw.nc'  # out file name for yearly data\n",
    "        # year_fp = f'{out_path}{year}/{year_fn}'  # out file path for yearly data(including file name)\n",
    "        # year_ds = xr.concat(month_list, dim='time')\n",
    "        # year_ds.to_netcdf(year_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d561ad-4fa0-460f-9935-1c020d44219f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mland_xr]",
   "language": "python",
   "name": "conda-env-.conda-mland_xr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
