{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e158896-bb9b-4409-9135-53a5fc849103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import numpy.matlib\n",
    "import datetime\n",
    "import xarray as xr\n",
    "from scipy import interpolate\n",
    "from numpy import ma\n",
    "from scipy import stats\n",
    "import scipy.io as sio\n",
    "import pickle as pickle\n",
    "from sklearn import linear_model\n",
    "import numpy.ma as ma\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import copy \n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# OS interaction\n",
    "import os\n",
    "import sys\n",
    "import cftime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import glob\n",
    "import dask\n",
    "import dask.bag as db\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from cartopy.crs import EqualEarth, PlateCarree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629946cd-81f1-432d-b51f-1ae716d16f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = '/glade/campaign/collections/rda/data/ds633.0/' # base path to ERA5 data on derecho\n",
    "out_path = '/glade/u/home/zcleveland/scratch/ERA5/dsw/' # base path to my subsetted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75548019-a034-4361-a9ae-bf3a5625e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other variables and their corresponding subdirectories here\n",
    "var_list = [\n",
    "    # 'lsp', # large scale precipitation (m of water) - accumu\n",
    "    # 'cp', # convective precipitation (m of water) - accumu\n",
    "    # 'sd', # snow depth  (m of water equivalent) - instan\n",
    "    # 'msl', # mean sea level pressure (Pa) - instan\n",
    "    # 'tcc', # total cloud cover (0-1) - instan\n",
    "    # 'stl1', # soil temp layer 1 (K) - instan\n",
    "    # 'stl2', # soil temp layer 2 (K) - instan\n",
    "    # 'stl3', # soil temp layer 3 (K) - instan\n",
    "    # 'stl4', # soil temp layer 4 (K) - instan\n",
    "    # 'swvl1', # soil volume water content layer 1 (m^3 m^-3) - instan\n",
    "    # 'swvl2', # soil volume water content layer 2 (m^3 m^-3) - instan\n",
    "    # 'swvl3', # soil volume water content layer 3 (m^3 m^-3) - instan\n",
    "    # 'swvl4', # soil volume water content layer 4 (m^3 m^-3) - instan\n",
    "    # '2t', # 2 meter temp (K) - instan\n",
    "    # '2d', # 2 meter dew point (K) - instan\n",
    "    # 'ishf', # instant surface heat flux (W m^-2) - instan\n",
    "    # 'ie', # instant moisture flux (kg m^-2 s^-1) - instan\n",
    "    # 'sshf', # surface sensible heat flux (J m^-2) - accumu\n",
    "    # 'slhf', # surface latent heat flux (J m^-2) - accumu\n",
    "    # 'ssr', # surface net solar radiation (J m^-2) - accumu\n",
    "    # 'str', # surface net thermal radiation (J m^-2) - accumu\n",
    "    # 'sro', # surface runoff (m) - accumu\n",
    "    # 'sf', # total snowfall (m of water equivalent) - accumu\n",
    "    # 'cape', # convective available potential energy (J kg^-1) - instan\n",
    "    'tcw', # total column water (kg m^-2) - sfc (sum total of solid, liquid, and vapor in a column)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4642ee-e180-48fa-93ce-9fd44476396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract, subset, and process data for single variable in the desert southwest\n",
    "def dsw_subset_era5(variable='lsp', start_date=200101, end_date=200102):\n",
    "    print(f'Processing variable: {variable}: {start_date}_{end_date}\\n')\n",
    "\n",
    "    # exit if not yearly data\n",
    "    if (end_date-start_date)>11: \n",
    "        print(f'Time range greater than 1 year. Skipping... ')\n",
    "        print(f'start_date: {start_date}\\n end_date: {end_date}\\n')\n",
    "        return\n",
    "    \n",
    "    start_time = time.time() # keep track of time to process.\n",
    "    # split start and end date to get year and month\n",
    "    start_year, start_month = f'{start_date}'[:4], f'{start_date}'[4:]\n",
    "    end_year, end_month = f'{end_date}'[:4], f'{end_date}'[4:]\n",
    "    # define output filename and path\n",
    "    out_fn = f'{variable}_{start_date}_{end_date}_dsw' # out file name\n",
    "    out_fp = f'{out_path}{start_year}/{out_fn}' # out file path (including file name)\n",
    "\n",
    "    # check if file already exists\n",
    "    if (os.path.exists(f'{out_fp}.nc') or \n",
    "        os.path.exists(f'{out_fp}_min.nc') or \n",
    "        os.path.exists(f'{out_fp}_max.nc') or\n",
    "        os.path.exists(f'{out_fp}_avg.nc')):\n",
    "        \n",
    "        print(f'File {out_fn} already exists. Skipping...\\n')\n",
    "        return\n",
    "    \n",
    "    # find the directory that the variable exists in\n",
    "    print(f'Searching for {var} in {era5_path}\\n')\n",
    "    contents = os.listdir(era5_path) # all contents in era5_path\n",
    "    # only get directories from era5_path\n",
    "    directories = [item for item in contents if os.path.isdir(os.path.join(era5_path, item))]\n",
    "    found = [] # initialize loop exit condition\n",
    "    for dir in directories:    \n",
    "        files = os.listdir(f'{era5_path}{dir}/{197901}') \n",
    "        for file in files:\n",
    "            if f'_{variable}.' in file: # check for existence of the var key in the file names\n",
    "                print(f'{var} found at {dir}\\n')\n",
    "                found.append(1)\n",
    "                var_dir = dir\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    if not found:\n",
    "        print(f'No files found with {variable}. Skipping...\\n')\n",
    "        return\n",
    "        \n",
    "    # find files for the variable in the specified directory\n",
    "    files = []\n",
    "    for year in range(int(start_year), int(end_year)+1): # input should be same year, so add 1 to end for \"range\" function\n",
    "        for month in range(1,13): # months 1-12 (jan-dec)\n",
    "            if month<10: # add a '0' to match date string format\n",
    "                year_month = f'{year}0{month}'\n",
    "            else:\n",
    "                year_month = f'{year}{month}'\n",
    "            try:\n",
    "                if ((f'{year_month}' < f'{start_date}') or (f'{year_month}' > f'{end_date}')):\n",
    "                    pass # in case the date is outside the range, just pass it\n",
    "                else:\n",
    "                    files += glob.glob(f'{era5_path}/{var_dir}/{year_month}/*_{variable}.*.nc', recursive=True)\n",
    "            except Exception as e:\n",
    "                print(f'Error in {era5_path}/{var_dir}/{year}0{month}/*_{variable}.*.nc: {e}\\n')\n",
    "    files.sort()\n",
    "\n",
    "    # calculate total number of directories for sanity check\n",
    "    total_directories = len(files)\n",
    "    print(f'{total_directories} number of files\\n')\n",
    "    \n",
    "    # create a list to hold data for each month\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}): # only for systems with dask\n",
    "        data_list = []\n",
    "        print(f'opening datasets')\n",
    "        data_list.append(xr.open_mfdataset(files))\n",
    "\n",
    "        # check if there are actually any files\n",
    "        if not data_list:\n",
    "            print(f'No files found for variable: {variable}')\n",
    "            print(f'\\rTime elapsed: {time.time()-start_time: .2f} s\\n')\n",
    "            return\n",
    "\n",
    "        # concatenate data for all months into single xarray dataset\n",
    "        print(f'Combining Data')\n",
    "        combined_data = xr.merge(data_list)\n",
    "\n",
    "        # subset the data for the Desert Southwest\n",
    "        lat_range = slice(40, 20) # lat range 20 N to 40 N\n",
    "        lon_range = slice(240, 260) # lon range 240 (120 W) to 260 (100 W)\n",
    "        ds_sub = combined_data.sel(latitude=lat_range, longitude=lon_range, drop=True)\n",
    "\n",
    "        # dimensions of accumulation data, instant data, etc.\n",
    "        acc_dims = ['forecast_hour', 'forecast_initial_time', 'latitude', 'longitude']\n",
    "        inst_dims = ['latitude', 'longitude', 'time']\n",
    "        \n",
    "        ### FOR ACCUMULCATIONS ###\n",
    "        if set(ds_sub.dims) == set(acc_dims):\n",
    "            # calculate sum total\n",
    "            daily_data = ds_sub.sum(dim='forecast_hour').resample(forecast_initial_time='1D').sum()\n",
    "            daily_data = daily_data.rename({'forecast_initial_time': 'time'}) # rename time dimension\n",
    "            # write data to NetCDF file\n",
    "            print(f'Writing data to NetCDF\\n')\n",
    "            daily_data.to_netcdf(f'{out_fp}.nc')\n",
    "\n",
    "        ### FOR DAILY AVERAGE, MIN, AND MAX ###\n",
    "        elif set(ds_sub.dims) == set(inst_dims):\n",
    "\n",
    "            temp = ds_sub.resample(time='1D') # turn into daily data\n",
    "\n",
    "            # find average and rename the variable to include AVG\n",
    "            daily_avg = temp.mean(dim='time')\n",
    "            var_xx = [varx for varx in daily_avg.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_avg = daily_avg.rename_vars({f'{var_xx}': f'{var_xx}_AVG'})\n",
    "\n",
    "            # find maximum and rename the variable to include MAX\n",
    "            daily_max = temp.max(dim='time')\n",
    "            var_xx = [varx for varx in daily_max.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_max = daily_max.rename_vars({f'{var_xx}': f'{var_xx}_MAX'})\n",
    "\n",
    "            # find minimum and rename the variable to include MIN\n",
    "            daily_min = temp.min(dim='time')\n",
    "            var_xx = [varx for varx in daily_min.data_vars.keys() if f'{variable.upper()}' in varx][0]\n",
    "            daily_min = daily_min.rename_vars({f'{var_xx}': f'{var_xx}_MIN'})\n",
    "\n",
    "            # combining data\n",
    "            print(f'Combining data')\n",
    "            daily_data = xr.merge([daily_avg,daily_max,daily_min], compat='override')\n",
    "            \n",
    "            # write data to NetCDF file\n",
    "            print(f'Writing data to NetCDF\\n')\n",
    "            daily_data.to_netcdf(f'{out_fp}.nc')\n",
    "            \n",
    "        else:\n",
    "            print(F'Dimensional error finding daily values for {var}') \n",
    "            print(f'Dimensions are {sorted(ds_sub.dims)}.\\n Skipping...\\n')\n",
    "            return\n",
    "\n",
    "\n",
    "        print(f'\\rTime elapsed: {time.time()-start_time: .2f} s\\n')\n",
    "\n",
    "    #return combined_data, ds_sub, ds_sub_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdbe023d-74c7-4058-98bb-b23928a0e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time array to loop through\n",
    "years = np.arange(1980,2020)\n",
    "months = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579cb813-938c-427f-a477-0a0ace606405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable: tcw: 198001_198012\n",
      "\n",
      "Searching for tcw in /glade/campaign/collections/rda/data/ds633.0/\n",
      "\n",
      "tcw found at e5.oper.an.sfc\n",
      "\n",
      "12 number of files\n",
      "\n",
      "opening datasets\n",
      "Combining Data\n",
      "Combining data\n",
      "Writing data to NetCDF\n",
      "\n",
      "Time elapsed:  61.80 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through variables in var_directories and process each one\n",
    "for var in var_list:\n",
    "    for year in years:\n",
    "        start_date = int(f'{year}01')\n",
    "        end_date = int(f'{year}12')\n",
    "        dsw_subset_era5(var, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43264879-0774-489f-9259-8caa1f0c983e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mland_xr]",
   "language": "python",
   "name": "conda-env-.conda-mland_xr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
