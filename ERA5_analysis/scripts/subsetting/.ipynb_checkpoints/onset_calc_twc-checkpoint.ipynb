{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7694fa3d-ab3e-427f-bb73-a34cd83a6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import numpy.matlib\n",
    "import datetime\n",
    "import xarray as xr\n",
    "from scipy import interpolate\n",
    "from numpy import ma\n",
    "from scipy import stats\n",
    "import scipy.io as sio\n",
    "import pickle as pickle\n",
    "from sklearn import linear_model\n",
    "import numpy.ma as ma\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from copy import copy \n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# OS interaction\n",
    "import os\n",
    "import sys\n",
    "import cftime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import glob\n",
    "import dask\n",
    "import dask.bag as db\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from cartopy.crs import EqualEarth, PlateCarree\n",
    "\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052e09d6-afd8-424d-a2db-c6c581c6f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_path = '/glade/u/home/zcleveland/scratch/ERA5/dsw/' # path to subsetted data\n",
    "sub_script_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/scripts/subsetting/' # path to subsetting scripts\n",
    "plot_script_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/scripts/plotting/' # path to plotting scripts\n",
    "fig_out_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/plots/' # path to generated figures\n",
    "temp_scratch_path = '/glade/u/home/zcleveland/NAM_soil-moisture/ERA5_analysis/temp/' # path to temp directory in scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37482965-703f-4d57-881f-22cac80d2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable list to choose\n",
    "var_list = [\n",
    "    # 'lsp', # large scale precipitation (m of water) - accumu\n",
    "    # 'cp', # convective precipitation (m of water) - accumu\n",
    "    # 'tp', # total precipitation (m of water) - accumu -- DERIVED\n",
    "    # 'sd', # snow depth  (m of water equivalent) - instan\n",
    "    # 'msl', # mean sea level pressure (Pa) - instan\n",
    "    # 'tcc', # total cloud cover (0-1) - instan\n",
    "    # 'stl1', # soil temp layer 1 (K) - instan\n",
    "    # 'stl2', # soil temp layer 2 (K) - instan\n",
    "    # 'stl3', # soil temp layer 3 (K) - instan\n",
    "    # 'stl4', # soil temp layer 4 (K) - instan\n",
    "    # 'swvl1', # soil volume water content layer 1 (m^3 m^-3) - instan\n",
    "    # 'swvl2', # soil volume water content layer 2 (m^3 m^-3) - instan\n",
    "    # 'swvl3', # soil volume water content layer 3 (m^3 m^-3) - instan\n",
    "    # 'swvl4', # soil volume water content layer 4 (m^3 m^-3) - instan\n",
    "    # '2t', # 2 meter temp (K) - instan\n",
    "    # '2d', # 2 meter dew point (K) - instan\n",
    "    # 'ishf', # instant surface heat flux (W m^-2) - instan\n",
    "    # 'ie', # instant moisture flux (kg m^-2 s^-1) - instan\n",
    "    # 'sshf', # surface sensible heat flux (J m^-2) - accumu\n",
    "    # 'slhf', # surface latent heat flux (J m^-2) - accumu\n",
    "    # 'ssr', # surface net solar radiation (J m^-2) - accumu\n",
    "    # 'str', # surface net thermal radiation (J m^-2) - accumu\n",
    "    # 'sro', # surface runoff (m) - accumu\n",
    "    # 'sf', # total snowfall (m of water equivalent) - accumu\n",
    "    # 'cape', # convective available potential energy (J kg^-1) - instan\n",
    "    'tcw', # total column water (kg m^-2) - sfc (sum total of solid, liquid, and vapor in a column)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5801b8c9-5d6d-436b-926c-c0bda8ab04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets for calculating onset timing\n",
    "\n",
    "# total water content stats\n",
    "tcw_max = xr.open_dataset(f'{data_in_path}tcw_max_stats.nc')\n",
    "tcw_min = xr.open_dataset(f'{data_in_path}tcw_min_stats.nc')\n",
    "\n",
    "# the average of the annual max/min daily total water content values\n",
    "pw_max = tcw_max['MEAN']\n",
    "pw_min = tcw_min['MEAN']\n",
    "\n",
    "# daily tcw values\n",
    "files = glob.glob(f'{data_in_path}*/tcw*.nc')\n",
    "files.sort()\n",
    "\n",
    "# open files and pull out daily average\n",
    "tcw = xr.open_mfdataset(files)\n",
    "tcw = tcw['TCW_AVG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c8f3a-6653-44f3-bc31-e69663d3a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate onset date using methods of Zeng and Lu 2004\n",
    "\n",
    "# normalized precipitable water index\n",
    "npwi = (tcw-pw_min)/(pw_max-pw_min)\n",
    "\n",
    "# set a threshold of 2*pi/10\n",
    "threshold = 2*np.pi/10\n",
    "\n",
    "# create mask for when npwi values exceed threshold\n",
    "def onset_condition(da):\n",
    "    # create a boolean mask where npwi exceeds the threshold\n",
    "    mask = da > threshold\n",
    "    \n",
    "    # use rolling window with length 3 along the time dimension and check if all values are True\n",
    "    return mask.rolling(time=3).sum() >= 3\n",
    "\n",
    "# apply onset condition across time dimension\n",
    "onset_mask = npwi.groupby('time.year').apply(onset_condition)\n",
    "\n",
    "# generate empty dataset to store \n",
    "onset_time = xr.Dataset(\n",
    "    coords={\n",
    "        'year': pd.date_range(start='1980-01-01', end='2019-01-01', freq='YS'),\n",
    "        'latitude': npwi.latitude.values,\n",
    "        'longitude': npwi.longitude.values\n",
    "    }\n",
    ")\n",
    "# create date variable\n",
    "dates = np.empty((40,81,81), dtype='datetime64[ns]')\n",
    "dates[:] = np.datetime64('NaT')  # store NaT values at temporary place holders\n",
    "onset_time['date'] = (('year', 'latitude', 'longitude'), dates)\n",
    "times = npwi.time\n",
    "\n",
    "for year in range(1980,2020):\n",
    "    print(f'\\n\\nYear: {year}')\n",
    "    temp_time = times.sel(time=str(year))\n",
    "    for lat in npwi.latitude:\n",
    "        print('\\n', end='')\n",
    "        if (lat%1==0):\n",
    "            print(f'\\nLat: {int(lat.values)} \\nLon: ')\n",
    "        for lon in npwi.longitude:\n",
    "            if (lon%5==0):\n",
    "                print(f'{int(lon.values)} ... ', end='')\n",
    "                \n",
    "            temp_mask = onset_mask.sel(time=str(year), latitude=lat, longitude=lon)\n",
    "            temp_idx = np.where(temp_mask)\n",
    "            temp_coord = {'year': str(year), 'latitude': lat, 'longitude': lon}\n",
    "\n",
    "            if temp_idx[0].size>0:\n",
    "                time = temp_time[temp_idx[0][0]]\n",
    "            else:\n",
    "                time = np.nan\n",
    "                \n",
    "            onset_time['date'].loc[temp_coord] = time\n",
    "\n",
    "# save as netcdf\n",
    "onset_time.to_netcdf(f'{data_in_path}NAM_onset.nc')\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9cb91b-462f-4b62-ae27-72d3f9e79e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_onset_trend(start_year, end_year):\n",
    "\n",
    "    # open dataset and extract time frame    \n",
    "    ds = xr.open_dataset(os.path.join(data_in_path, 'NAM_onset.nc'))\n",
    "    onset_time = ds['date'].sel(year=slice(str(start_year), str(end_year)), drop=True)\n",
    "    ds.close()\n",
    "\n",
    "    # convert from datetime to ordinal day\n",
    "    onset_day = onset_time.dt.dayofyear\n",
    "\n",
    "    # compute the gradient\n",
    "    grad = onset_day.diff('year')\n",
    "    # print(grad.shape)\n",
    "    # print(grad)\n",
    "\n",
    "    # compute gradient ignoring with nan mask\n",
    "    nan_mask = xr.where(onset_day.isnull(), True, False)\n",
    "    onset_masked = onset_day.where(~nan_mask)\n",
    "    nan_grad = onset_masked.diff('year')\n",
    "\n",
    "    # compute mean gradients\n",
    "    mean_grad = grad.mean(dim='year')\n",
    "    mean_nan_grad = nan_grad.mean(dim='year')\n",
    "    \n",
    "    return grad, nan_grad, mean_grad, mean_nan_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2e6c99-786f-41e5-a8fa-7a3028947100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some arrays to loop through and calculate trends\n",
    "# sy10 = [1980, 1990, 2000, 2010]  # 10 year trends\n",
    "# ey10 = [1989, 1999, 2009, 2019]\n",
    "\n",
    "# sy20 = [1980, 2000]  # 20 year trends\n",
    "# ey20 = [1999, 2019]\n",
    "\n",
    "sy40 = 1980  # 40 year trend\n",
    "ey40 = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95c7861-6a1c-4906-9d44-070e149ad0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad, nan_grad, mean_grad, mean_nan_grad = calc_onset_trend(sy40, ey40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ecbe77-536d-4cb0-9d96-8d353e2344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad.to_netcdf(f'{data_in_path}onset_gradient.nc')\n",
    "# mean_grad.to_netcdf(f'{data_in_path}onset_mean_gradient.nc')\n",
    "# nan_grad.to_netcdf(f'{data_in_path}onset_nan_gradient.nc')\n",
    "# mean_nan_grad.to_netcdf(f'{data_in_path}onset_mean_nan_gradient.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba88193-955e-47ea-8702-170c809535c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average onset day\n",
    "\n",
    "# get files and open them.\n",
    "files = os.path.join(data_in_path, 'NAM_onset.nc')\n",
    "ds = xr.open_dataset(files)\n",
    "da = ds['date']\n",
    "\n",
    "onset_mean = da.mean(dim='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23802612-06ee-4d67-a48d-34cf50746b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mland_xr]",
   "language": "python",
   "name": "conda-env-.conda-mland_xr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
